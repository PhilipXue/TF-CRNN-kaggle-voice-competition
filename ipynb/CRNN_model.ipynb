{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from collections import namedtuple\n",
    "slim = tf.contrib.slim\n",
    "keras = tf.keras\n",
    "GRU = keras.layers.GRU\n",
    "BIO_DIR = keras.layers.Bidirectional\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv = namedtuple('Conv', ['kernel', 'stride', 'filters','pooling'])\n",
    "Gru = namedtuple('Gru',['nn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_config = [\n",
    "    Conv(kernel=(5,5),stride=(1,1),filters=96,pooling=(4,1)),\n",
    "    Conv(kernel=(5,5),stride=(1,1),filters=96,pooling=(4,1)),\n",
    "    Conv(kernel=(5,5),stride=(1,1),filters=96,pooling=(4,1)),\n",
    "    Conv(kernel=(5,5),stride=(1,1),filters=96,pooling=(2,1)),\n",
    "]\n",
    "rnn_config = [\n",
    "    Gru(nn=96),\n",
    "    Gru(nn=96)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(object):\n",
    "    def __init__(self,input_images, conv_config,rnn_config,class_num,bio_direct):\n",
    "        self.net = input_images\n",
    "        self.size = input_images.shape\n",
    "        self.conv_config = conv_config\n",
    "        self.rnn_config = rnn_config\n",
    "        self.bio_direct = bio_direct\n",
    "        self.class_num = class_num\n",
    "    def build_conv_layers(self):\n",
    "        for i,config in enumerate(self.conv_config):\n",
    "            self.net = slim.conv2d(self.net,config.filters,\n",
    "                                   kernel_size=config.kernel,\n",
    "                                   stride=config.stride,\n",
    "                                   normalizer_fn=slim.batch_norm)\n",
    "            print(self.net.get_shape())\n",
    "            self.net  = slim.max_pool2d(self.net,config.pooling,stride=config.pooling)\n",
    "            print(self.net.get_shape())\n",
    "        self.net = tf.squeeze(self.net,axis=[1])\n",
    "        print(self.net.get_shape())\n",
    "    def build_rnn_layer(self):\n",
    "        for i, config in enumerate(self.rnn_config):\n",
    "            self.net = GRU(config.nn,return_sequences=True)(self.net)\n",
    "#             if bio_direct:\n",
    "                \n",
    "                \n",
    "#             else:\n",
    "#                 self.net = GRU(config.nn,return_sequences=True)(self.net)\n",
    "\n",
    "        self.net = keras.layers.MaxPool1D(pool_size=[self.size[2]])(self.net)\n",
    "        self.net = tf.squeeze(self.net,[1])\n",
    "        print(self.net.get_shape())\n",
    "    def build_pooling_fc(self):\n",
    "        # always name the last layer as final layer\n",
    "        self.final_layer = slim.fully_connected(self.net,self.class_num,activation_fn=tf.nn.softmax)\n",
    "        print(self.final_layer.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/home/philip/data/Keyword_spot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "valid_on_batch = 100\n",
    "save_on_batch = 5000\n",
    "total_step = 20000\n",
    "class_num=31\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_direct = True\n",
    "# ohem = online hard example mining\n",
    "# This is to select examples within a mini-batch with the top-k losses and only BP the loss on them\n",
    "# ratio is to determin how many example to BP\n",
    "ohem = True\n",
    "ohem_ratio_start = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 107488 images belonging to 31 classes.\n",
      "Found 14428 images belonging to 31 classes.\n"
     ]
    }
   ],
   "source": [
    "training_data_gen = keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1)\n",
    "training_data_set_dir = os.path.join(data_root,\"train/train_on_all/\")\n",
    "# training_data_set_dir = os.path.join(data_root,\"train_on_all\")\n",
    "\n",
    "training_gen = training_data_gen.flow_from_directory(training_data_set_dir,class_mode=\"categorical\",\n",
    "                                                     target_size=(128,63),color_mode=\"grayscale\",\n",
    "                                                     batch_size=batch_size)\n",
    "valid_dataset_dir = os.path.join(data_root,\"train/valid_image\")\n",
    "valid_data_gen = keras.preprocessing.image.ImageDataGenerator().flow_from_directory(valid_dataset_dir,\n",
    "                                                                                    class_mode=\"categorical\",\n",
    "                                                                                    target_size=(128,63),\n",
    "                                                                                    color_mode=\"grayscale\",\n",
    "                                                                                    batch_size=batch_size)\n",
    "\n",
    "training_gen.reset()\n",
    "valid_data_gen.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network(conv_config,rnn_config,class_num,bio_direct):\n",
    "    input_images = tf.placeholder(tf.float32,[None,128,63,1],name = 'input')\n",
    "    crnn = CRNN(input_images,conv_config,rnn_config,class_num,bio_direct)\n",
    "    crnn.build_conv_layers()\n",
    "    crnn.build_rnn_layer()\n",
    "    crnn.build_pooling_fc()\n",
    "    crnn.predictions = tf.nn.l2_normalize(crnn.final_layer, 1, 1e-10, name='predicitons')\n",
    "    return crnn,input_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128, 63, 96)\n",
      "(?, 32, 63, 96)\n",
      "(?, 32, 63, 96)\n",
      "(?, 8, 63, 96)\n",
      "(?, 8, 63, 96)\n",
      "(?, 2, 63, 96)\n",
      "(?, 2, 63, 96)\n",
      "(?, 1, 63, 96)\n",
      "(?, 63, 96)\n",
      "(?, 96)\n",
      "(?, 31)\n"
     ]
    }
   ],
   "source": [
    "crnn,input_images = init_network(conv_config,rnn_config,class_num,bio_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = 0\n",
    "total_step = 5000\n",
    "save_model = False\n",
    "valid_on_batch = 100\n",
    "# class_num = 30\n",
    "save_on_batch = 5000\n",
    "# ohem = online hard example mining\n",
    "# This is to select examples within a mini-batch with the top-k losses and only BP the loss on them\n",
    "# ratio is to determin how many example to BP\n",
    "ohem = True\n",
    "ohem_ratio_start = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.placeholder(tf.float32,[None,class_num],name='label')\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "diff = keras.losses.categorical_crossentropy(labels,crnn.predictions)\n",
    "v_loss = tf.reduce_mean(diff)\n",
    "if ohem:\n",
    "    ohem_ratio = tf.train.exponential_decay(ohem_ratio_start,global_step,1000,0.9,staircase=True)\n",
    "    k = tf.cast(tf.multiply(ohem_ratio,tf.cast(tf.shape(diff)[0],tf.float32)),dtype=tf.int32)\n",
    "    diff,indices = tf.nn.top_k(diff,k=k)\n",
    "loss =tf.reduce_mean(diff)\n",
    "acc = tf.reduce_mean(keras.metrics.categorical_accuracy(labels, crnn.predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_learning_rate = 1e-2\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "# learning_rate = tf.train.natural_exp_decay(starter_learning_rate,global_step,5000,0.3)\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate,global_step,5000,0.2,staircase=True)\n",
    "# learning_rate = starter_learning_rate\n",
    "learning_rate = tf.maximum(learning_rate,1e-5)\n",
    "# train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "# train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_sum = tf.summary.scalar(\"loss\",loss)\n",
    "learning_rate_sum = tf.summary.scalar(\"learning_rate\",learning_rate)\n",
    "train_acc_sum = tf.summary.scalar(\"classification_accuarcy\",acc)\n",
    "now = time.localtime(time.time())\n",
    "time_folder = \"%d_%d_%d_%d_%d\"%(now.tm_mon,now.tm_mday,now.tm_hour,now.tm_min,now.tm_sec)\n",
    "training_summary_merge = tf.summary.merge([train_loss_sum,learning_rate_sum,train_acc_sum])\n",
    "log_dir = os.path.join(data_root,\"CRNN/log/\")\n",
    "log_dir_training = os.path.join(log_dir+ \"train/\")\n",
    "tb_writer＿training = tf.summary.FileWriter(logdir=log_dir_training+time_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_valid = os.path.join(log_dir+ \"valid/\")\n",
    "tb_writer＿valid = tf.summary.FileWriter(logdir=log_dir_valid+time_folder)\n",
    "# valid_loss_sum = tf.summary.scalar(\"loss\",v_loss)\n",
    "# valid_acc_sum = tf.summary.scalar(\"classification_accuarcy\",acc)\n",
    "# valid_summary_merge = tf.summary.merge([valid_loss_sum,valid_acc_sum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init_op = tf.group(\n",
    "        tf.local_variables_initializer(),\n",
    "        tf.global_variables_initializer())\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_new_sum(writer,value,name,g_step):\n",
    "    summary = tf.Summary()\n",
    "    new_sum = summary.value.add()\n",
    "    new_sum.simple_value = value\n",
    "    new_sum.tag = name\n",
    "    tb_writer_valid.add_summary(summary,g_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "valid_on_batch = 200\n",
    "total_step = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd83f35baa864519b2e924af6b3da1e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model_save_folder = os.path.join(data_root,\"CRNN/model\",time_folder)\n",
    "# os.mkdir(model_save_folder)\n",
    "for step in tqdm.tqdm_notebook(range(total_step)):\n",
    "    image_batch,label_batch = training_gen.__next__()\n",
    "    loss_get,_,training_summary = sess.run([loss,train_op,training_summary_merge],feed_dict={input_images:image_batch,labels:label_batch})\n",
    "    g_step = tf.train.global_step(sess,global_step)\n",
    "    tb_writer_training.add_summary(training_summary,g_step)\n",
    "    if valid and (g_step+1)%valid_on_batch == 0:\n",
    "        counter = 01\n",
    "        valid_losses = []\n",
    "        valid_acces = []\n",
    "        for i in range(valid_data_gen.samples//valid_data_gen.batch_size+1):\n",
    "            valid_images,valid_labels =valid_data_gen.next()\n",
    "            valid_loss, valid_acc= sess.run([v_loss,acc],feed_dict={input_images:valid_images,labels:valid_labels})\n",
    "            counter += 1\n",
    "            valid_losses.append(valid_loss)\n",
    "            valid_acces.append(valid_acc)\n",
    "#         print(np.mean(valid_losses))\n",
    "        write_new_sum(tb_writer_valid,np.mean(valid_losses),\"loss\",g_step)\n",
    "        write_new_sum(tb_writer_valid,np.mean(valid_acces),\"classification_accuarcy\",g_step)\n",
    "    \n",
    "    if save_model and (g_step+1)%save_on_batch == 0:\n",
    "        save_path = os.path.join(model_save_folder,str(g_step+1),\"mobilenet.ckpt\")\n",
    "        os.mkdir(os.path.dirname(save_path))\n",
    "        save_path = saver.save(sess,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indice = list(training_gen.class_indices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "im = cv2.imread(data_root+\"test/test_image/1aed7c6d_nohash_0.png\",0)\n",
    "im = im.reshape((1,128,63,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sess.run([crnn.predictions],feed_dict={input_images:im})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nine'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_indice[np.argmax(a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_command = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'silence', 'unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import tqdm\n",
    "import csv\n",
    "batch_size = 128\n",
    "csvfile = open(\"new_submission.csv\",'w')\n",
    "fieldnames = ['fname', 'label']\n",
    "writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/home/philip/data/Keyword_spot/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_and_save_result(file_name,predict):\n",
    "    if predict not in valid_command:\n",
    "        predict = \"unknown\"\n",
    "    filen = (os.path.basename(file_name)).replace(\"png\",\"wav\")\n",
    "    writer.writerow({\"fname\":filen,\"label\":predict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d9c06d6046462c9c625c1c9a9a8457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_files = glob.glob(data_root+\"test/test_image/*.png\")\n",
    "total_file = len(test_files)\n",
    "for start in tqdm.tqdm_notebook(range(0,total_file,batch_size)):\n",
    "    end = min(start+batch_size,total_file)\n",
    "    batch_files =test_files[start:end]\n",
    "    im = np.array([cv2.imread(i,0) for i in batch_files])\n",
    "    im = np.expand_dims(im,-1)\n",
    "#     print(im.shape)\n",
    "#     im = im.reshape((batch_size,128,63,1))\n",
    "    result = sess.run(crnn.predictions,feed_dict={input_images:im})\n",
    "#     print(len(result[0][1]))\n",
    "    predicts = [class_indice[np.argmax(a)] for a in result]\n",
    "    [replace_and_save_result(_[0],_[1]) for _ in zip(batch_files,predicts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thought at the point\n",
    "- CRNN composed of CNN feature extractor and RNN, try different number of layers and layer configurations\n",
    "- still don't understand how to convert CNN output to RNN input (how to convert feature map into rnn input)\n",
    "- elimenate the audio files that are not correctly recored\n",
    "- try online hard example mining and offline\n",
    "    - online: select the top-k loss to bp\n",
    "    - offline: examples could not be detected by existing model\n",
    "- try biodirection\n",
    "- try Pure CNN\n",
    "- find keyword spotting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# model = model.load(os.path.join(data_root,\"saved_model/12_27/CRNN_20000.pb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Model()\n",
    "model.predict_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def training_network(network,input_images,class_num,training_step):\n",
    "#     labels = tf.placeholder(tf.float32,[None,class_num],name='label')\n",
    "#     loss = tf.reduce_mean(keras.losses.categorical_crossentropy(labels,network.predictions),name='training_loss')\n",
    "#     acc = tf.reduce_mean(keras.metrics.categorical_accuracy(labels, network.predictions))\n",
    "# #     network.predictions = tf.cast(network.predictions,tf.float32)\n",
    "#     labels_logit = tf.argmax(labels,axis=1)\n",
    "#     predictions_logits = tf.argmax(network.predictions,axis=1)\n",
    "#     acc = tf.reduce_mean(tf.cast(tf.equal(labels_logit,predictions_logits),tf.float32))\n",
    "# #     acc,_ = tf.metrics.accuracy(labels,network.predictions)\n",
    "# #     train_acc = tf.summary.scalar(\"classification_accurcy/training\",acc)\n",
    "#     train_op = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "    \n",
    "#     sess = tf.InteractiveSession()\n",
    "#     init_op = tf.group(\n",
    "#             tf.local_variables_initializer(),\n",
    "#             tf.global_variables_initializer())\n",
    "#     sess.run(init_op)\n",
    "    \n",
    "#     loss_his = []\n",
    "#     acc_his = []\n",
    "#     for step in tqdm.tqdm(range(training_step)):\n",
    "#         batch_image , batch_label = training_gen.__next__()\n",
    "#         _,pre_loss,t_acc = sess.run([train_op,loss,acc],feed_dict={input_images:batch_image,labels:batch_label})\n",
    "# #         print(t_acc)\n",
    "#         loss_his.append(pre_loss)\n",
    "#         acc_his.append(t_acc)\n",
    "#     return loss_his,acc_his"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "state": {
    "89005b768e374dc6a10e896f0db5833d": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "d0665ff8b1204c4396e9f0e6f877c405": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
